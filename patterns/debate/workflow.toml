[workflow]
name = "debate"
description = "Adversarial multi-tool AI debate for strategy formulation with tier-based model escalation"

[[workflow.variables]]
name = "CRITIC_MODEL"

[[workflow.variables]]
name = "CRITIQUE_PROMPT"

[[workflow.variables]]
name = "CURRENT_TIER"

[[workflow.variables]]
name = "ESCALATION_PROMPT"

[[workflow.variables]]
name = "HIGHER_TIER_MODEL"

[[workflow.variables]]
name = "NEEDS_ESCALATION"

[[workflow.variables]]
name = "PROPOSAL_PROMPT"

[[workflow.variables]]
name = "PROPOSER_MODEL"

[[workflow.variables]]
name = "RESPONSE_PROMPT"

[[workflow.steps]]
id = 1
title = "Role Detection"
prompt = """
Determine if this agent is the orchestrator or a debate participant.
If initial prompt contains "Use the debate skill" → participant mode.
If invoked by user via /debate → orchestrator mode.
Participants MUST NOT run any csa commands (infinite recursion)."""
on_fail = "abort"

[[workflow.steps]]
id = 2
title = "Verify Prerequisites"
tool = "bash"
prompt = """
Verify csa binary is available and tiers are configured.

```bash
which csa && csa --format json tiers list
```"""
on_fail = "abort"

[[workflow.steps]]
id = 3
title = "Discover Available Models"
tool = "bash"
prompt = """
Parse tiers JSON to get available models. At least one tier needs
>= 2 models for debate. Record ordered tier list for escalation.

```bash
csa --format json tiers list
```"""
on_fail = "abort"

[[workflow.steps]]
id = 4
title = "Resolve Debate Tool"
prompt = """
CSA auto-selects heterogeneous tool: claude-code caller → codex reviewer,
codex caller → claude-code reviewer. Override with explicit --tool if needed."""
on_fail = "abort"

[[workflow.steps]]
id = 5
title = "Select Starting Tier"
prompt = """
Use tier mapped to default in tier_mapping, or user-specified tier.
Filter models to those matching the debate tool.
Validate >= 2 models available. If < 2, try next higher tier."""
on_fail = "abort"

[[workflow.steps]]
id = 6
title = "Round 1 — Proposal"
tool = "csa"
prompt = """
Proposer presents concrete, actionable strategy with:
1. Core Strategy (2-3 sentences)
2. Key Arguments (numbered, with evidence)
3. Implementation Steps (concrete actions)
4. Anticipated Weaknesses (honest limitations)

```bash
csa run --model-spec "${PROPOSER_MODEL}" --ephemeral "${PROPOSAL_PROMPT}"
```"""
tier = "${CURRENT_TIER}"
on_fail = "abort"

[[workflow.steps]]
id = 7
title = "Round 1 — Critique"
tool = "csa"
prompt = """
Critic rigorously evaluates the proposal:
1. Logical Flaws
2. Missing Considerations
3. Better Alternatives
4. Strongest Counter-Arguments

```bash
csa run --model-spec "${CRITIC_MODEL}" --ephemeral "${CRITIQUE_PROMPT}"
```"""
tier = "${CURRENT_TIER}"
on_fail = "abort"

[[workflow.steps]]
id = 8
title = "Round 1 — Response"
tool = "csa"
prompt = """
Proposer responds to each criticism:
1. Concede valid points and revise strategy
2. Refute invalid criticisms with evidence
3. Present revised strategy

```bash
csa run --model-spec "${PROPOSER_MODEL}" --ephemeral "${RESPONSE_PROMPT}"
```"""
tier = "${CURRENT_TIER}"
on_fail = "abort"

[[workflow.steps]]
id = 9
title = "Convergence Evaluation"
prompt = """
Orchestrator evaluates after each critique-response pair:
- Both sides agree on core strategy → end debate
- Arguments repeat without novel insights → end debate
- Proposer cannot counter critique → escalate tier
- Arguments circular → escalate tier"""
on_fail = "abort"

[[workflow.steps]]
id = 10
title = "Tier Escalation"
prompt = """
Find next higher tier. Summarize debate so far as context.
Restart debate loop with higher tier models.
Max 2 escalations.

```bash
csa run --model-spec "${HIGHER_TIER_MODEL}" --ephemeral "${ESCALATION_PROMPT}"
```"""
on_fail = "abort"
condition = "${NEEDS_ESCALATION}"

[[workflow.steps]]
id = 11
title = "Final Synthesis"
prompt = """
Produce debate result document with:
- Final Strategy
- Key Insights from Debate
- Resolved Tensions
- Remaining Uncertainties
- Debate Trajectory (tiers, rounds, models used)
- Full model specs for ALL participants (audit trail)"""
on_fail = "abort"
